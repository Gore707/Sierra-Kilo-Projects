import subprocess
import requests
from bs4 import BeautifulSoup
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Install required dependencies using pip
subprocess.call(['pip', 'install', 'tensorflow', 'scikit-learn', 'transformers', 'requests', 'beautifulsoup4'])

# Define a function to collect data related to various topics from Wikipedia
def collect_data():
    try:
        urls = [
            "Henry_Kissinger",  # Henry Kissinger
            "Vietnam_War",      # Vietnam War
            "Russian_invasion_of_Ukraine",  # Russian invasion of Ukraine
            "COVID-19",         # COVID-19
            "List_of_CIA_controversies",    # List of CIA controversies
            "Modern_warfare",   # Modern warfare
            "Espionage",        # Espionage
            "Nuclear_warfare",  # Nuclear warfare
            "Mutual_assured_destruction",   # Mutual assured destruction
            "Republican_Party_(United_States)",  # Republican Party (United States)
            "Communism",        # Communism
            "Barack_Obama",     # Barack Obama
            "Climate_change",   # Climate change
            "Space_exploration",  # Space exploration
            "Artificial_intelligence",  # Artificial intelligence
            "World_War_II",     # World War II
            "European_Union",   # European Union
            "Global_warming",   # Global warming
            "Human_rights",     # Human rights
            "International_relations",  # International relations
            "Capitalism"        # Capitalism
        ]
        all_data = []
        base_url = "https://en.wikipedia.org/wiki/"
        for topic in urls:
            url = base_url + topic
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                paragraphs = soup.find_all('p')
                data = [p.text.strip() for p in paragraphs if p.text.strip()]
                all_data.extend(data)
        return all_data
    except Exception as e:
        print(f"Error occurred during data collection: {e}")
        return []

# Define a function to preprocess the collected data
def preprocess_data(data):
    try:
        preprocessed_data = [text.replace('\n', ' ') for text in data]
        return preprocessed_data
    except Exception as e:
        print(f"Error occurred during data preprocessing: {e}")
        return []

# Collect and preprocess data related to various topics
data = collect_data()
preprocessed_data = preprocess_data(data)

# Define the target labels for the collected data
labels = [
    'Henry Kissinger', 'Vietnam War', 'Russian invasion of Ukraine', 'COVID-19',
    'List of CIA controversies', 'Modern warfare', 'Espionage', 'Nuclear warfare',
    'Mutual assured destruction', 'Republican Party (United States)', 'Communism',
    'Barack Obama', 'Climate change', 'Space exploration', 'Artificial intelligence',
    'World War II', 'European Union', 'Global warming', 'Human rights', 'International relations',
    'Capitalism'
]

# Define target labels for collected data
y = [label for _ in range(len(data)) for label in labels]

# Additional Data
additional_X = [
    "Peace is more important than victory.",
    "The absence of alternatives clears the mind marvelously.",
    "The task of the leader is to get his people from where they are to where they have not been.",
    "Power is the ultimate aphrodisiac.",
    "Ninety percent of the politicians give the other ten percent a bad reputation.",
    "The illegal we do immediately. The unconstitutional takes a little longer.",
    "Accept everything about yourself — I mean everything, You are you and that is the beginning and the end — no apologies, no regrets.",
    "America has no permanent friends or enemies, only interests.",
    "A diamond is a chunk of coal that did well under pressure.",
    "History knows no resting places and no plateaus.",
    "The longer I am out of office, the more infallible I appear to myself.",
    "The American temptation is to believe that foreign policy is a subdivision of psychiatry.",
    "Even a paranoid can have enemies.",
    "I would rather be the emperor of my own domain than the emperor of someone else's.",
    "There cannot be a crisis next week. My schedule is already full.",
    "Nobody will ever win the battle of the sexes. There's too much fraternizing with the enemy.",
    "Military men are just dumb, stupid animals to be used as pawns in foreign policy.",
    "If it's going to come out eventually, better have it come out immediately.",
    "Each success only buys an admission ticket to a more difficult problem.",
    "The test of policy is how well it stands up to adversity.",
    "A leader does not deserve the name unless he is willing occasionally to stand alone.",
    "People are generally amazed that I would take an interest in any form that would require me to stop talking for three hours.",
    "Moderation is a virtue only in those who are thought to have an alternative.",
    "The statesman's duty is to bridge the gap between his nation's experience and his vision."
]

additional_y = [
    'Diplomacy', 'Realpolitik', 'Leadership', 'Power', 'Politics', 'Strategy', 'Identity, Espionage',
    'Peace', 'Leadership', 'Power', 'Politics', 'Strategy', 'Politics', 'Identity',
    'Peace', 'Leadership', 'Power', 'Politics', 'Strategy', 'Politics', 'Identity',
    'Peace', 'Leadership', 'Power', 'Politics', 'Strategy', 'Politics', 'Identity',
    'Peace', 'Leadership', 'Power', 'Politics', 'Strategy', 'Politics', 'Identity',
    'Peace', 'Leadership', 'Power', 'Politics', 'Strategy', 'Politics', 'Identity',
    'Peace', 'Leadership', 'Power', 'Politics', 'Strategy', 'Politics', 'Identity',
    'Diplomacy', 'Realpolitik', 'Leadership', 'Power', 'Politics', 'Strategy', 'Identity, Espionage',
]

# Append additional data and labels to the existing data and labels
preprocessed_data.extend(additional_X)
y.extend(additional_y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, y, test_size=0.2, random_state=42)

# Define maximum number of words and maximum sequence length for tokenization
max_words = 10000
maxlen = 200

# Tokenize the text data
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)
X_train_tokenized = tokenizer.texts_to_sequences(X_train)
X_test_tokenized = tokenizer.texts_to_sequences(X_test)

# Pad sequences to ensure uniform input size
X_train_padded = pad_sequences(X_train_tokenized, maxlen=maxlen, padding='post')
X_test_padded = pad_sequences(X_test_tokenized, maxlen=maxlen, padding='post')

# Encode target labels
label_encoder = LabelEncoder()
label_encoder.fit(y_train)
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)
num_classes = len(label_encoder.classes_)

# Convert target labels to one-hot encoding
y_train_one_hot = to_categorical(y_train_encoded, num_classes=num_classes)
y_test_one_hot = to_categorical(y_test_encoded, num_classes=num_classes)

# Build the LSTM model architecture
embedding_dim = 100
model = Sequential([
    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen),
    LSTM(128, dropout=0.2, recurrent_dropout=0.2),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
try:
    history = model.fit(X_train_padded, y_train_one_hot, epochs=5, batch_size=32, validation_split=0.2)
except Exception as e:
    print(f"Error occurred during model training: {e}")

# Save the model and tokenizer
try:
    model.save('text_classification_model.h5')
    tokenizer.save('tokenizer.h5')
except Exception as e:
    print(f"Error occurred while saving model or tokenizer: {e}")

# Evaluate the model
try:
    loss, accuracy = model.evaluate(X_test_padded, y_test_one_hot)
    print(f"Test Loss: {loss}")
    print(f"Test Accuracy: {accuracy}")
except Exception as e:
    print(f"Error occurred during model evaluation: {e}")

# Plot training history
try:
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()
except Exception as e:
    print(f"Error occurred while plotting training history: {e}")

# Function to interact with the model
def talk_to_model(text):
    try:
        # Tokenize and pad the input text
        input_text = [text]
        input_tokenized = tokenizer.texts_to_sequences(input_text)
        input_padded = pad_sequences(input_tokenized, maxlen=maxlen, padding='post')

        # Predict the category
        prediction = model.predict(input_padded)
        predicted_category_index = prediction.argmax()
        predicted_category = label_encoder.inverse_transform([predicted_category_index])[0]
        
        return predicted_category
    except Exception as e:
        print(f"Error occurred during inference: {e}")
        return None

# Example usage
user_input = input("Ask a question or provide a statement: ")
predicted_category = talk_to_model(user_input)
if predicted_category:
    print(f"The predicted category for your input is: {predicted_category}")
else:
    print("Failed to predict the category.")
